{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Includes main algorithm to calculate OTDGWD between datasets, the relevant comparison between distance and transferability error along with the associated graphs and figures\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import statsmodels.api as sm\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition)\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import cluster\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import offsetbox\n",
    "import math\n",
    "import ot\n",
    "import pickle\n",
    "import pandas as pd\n",
    "%store -r\n",
    "from emnist import extract_training_samples\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#loading transformed MNIST Datasets (see dataaugment.ipynb for details)\n",
    "with open('dataaugment1.pickle', 'rb') as f:\n",
    "    data_cropp, data_60, data_90, data_120, data_150, data_180= pickle.load(f,encoding='bytes')\n",
    "\n",
    "#loading USPS Dataset\n",
    "path_to_data = \"./USPSdata/Numerals/\"\n",
    "def resize_and_scale(img, size):\n",
    "    img = cv2.resize(img, size)\n",
    "    return 1 - np.array(img, \"float32\")\n",
    "\n",
    "img_list = os.listdir(path_to_data)\n",
    "sz = (16,16)\n",
    "validation_usps = []\n",
    "validation_usps_label = []\n",
    "for i in range(10):\n",
    "    label_data = path_to_data + str(i) + '/'\n",
    "    img_list = os.listdir(label_data)\n",
    "    for name in img_list:\n",
    "        if '.png' in name:\n",
    "            img = cv2.imread(label_data+name)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized_img = resize_and_scale(img, sz)\n",
    "            validation_usps.append(resized_img.flatten())\n",
    "            validation_usps_label.append(i)\n",
    "\n",
    "validation_usps = np.array(validation_usps)\n",
    "\n",
    "validation_usps_label= np.array(validation_usps_label)\n",
    "\n",
    "#loading KMNIST\n",
    "kmnist_data = np.load(r'C:\\Users\\IST\\Downloads\\kmnist-train-imgs.npz')\n",
    "\n",
    "kmnist_data = kmnist_data.f.arr_0\n",
    "\n",
    "labels_kmnist = np.load(r'C:\\Users\\IST\\Downloads\\kmnist-train-labels.npz')\n",
    "\n",
    "labels_kmnist = labels_kmnist.f.arr_0\n",
    "\n",
    "#loading MNIST\n",
    "mnistt = fetch_openml('mnist_784')\n",
    "#Loading FASHION-MNIST\n",
    "fmnistt = fetch_openml('Fashion-MNIST') \n",
    "# loading EMNIST\n",
    "images, labels = extract_training_samples('letters') #emnist\n",
    "\n",
    "#get mean and covariance for each label distribution in the dataset\n",
    "def label_params(df , target):\n",
    "    feature_label_pairs = df.loc[(df['target' ]== target)]\n",
    "    feature_label_pairs = feature_label_pairs.iloc[:,0:len(df.columns)-1]\n",
    "    features = pd.DataFrame.to_numpy(feature_label_pairs)\n",
    "\n",
    "    sum_mean = np.zeros(len(df.columns)-1)\n",
    "    for i in range(len(features)-1):\n",
    "        sum_mean = sum_mean + features[i]\n",
    "    \n",
    "    \n",
    "    mu = sum_mean / len(features)\n",
    "\n",
    "    sum_cov = np.zeros((len(df.columns)-1, len(df.columns)-1))\n",
    "    for i in range(len(features)-1):\n",
    "        sum_cov = sum_cov + np.outer(np.transpose(features[i]-mu), (features[i]-mu))\n",
    "\n",
    "    \n",
    "    cov = sum_cov / len(features)\n",
    "    \n",
    "    return [mu , cov]\n",
    "\n",
    "#get distance between two labels\n",
    "def label_dist(params_1 , params_2):\n",
    "    w_2 = np.linalg.norm(params_1[0] - params_2[0]) + np.matrix.trace(params_1[1] + params_2[1] - 2*np.sqrt((np.sqrt(params_1[1])*(params_2[1])*np.sqrt(params_1[1]))))\n",
    "    return w_2\n",
    "\n",
    "#store mean and covariance for each label in a dictionary\n",
    "def class_param(target, df):\n",
    "    class_params = dict.fromkeys(np.unique(target))\n",
    "    for key in class_params.keys():\n",
    "       class_params[key] = label_params(df, key) \n",
    "    return class_params\n",
    "\n",
    "#get pointwise pairwise distances between points in a dataset\n",
    "def cost_mat(data, target , df ):\n",
    "    print('here')\n",
    "    class_params = class_param(target, df)\n",
    "    cost_mat = np.zeros(( len(df), len(df) ))\n",
    "    for i in range(0,len(df)):\n",
    "#          print(i)\n",
    "         target_1 = target[i]\n",
    "         params_1 = class_params[target_1]\n",
    "         for j in range(0,len(df)):\n",
    "#                 print(j)\n",
    "                target_2 = target[j]\n",
    "                params_2 = class_params[target_2]\n",
    "                w_2 = label_dist(params_1, params_2)\n",
    "                cdist = np.linalg.norm(data[i] - data[j])\n",
    "            \n",
    "            \n",
    "                if (cdist + w_2) >= 0:\n",
    "                      pairwise_dist = math.sqrt(cdist + w_2)\n",
    "                else:\n",
    "                      pairwise_dist = 0.0\n",
    "        \n",
    "                cost_mat[i][j] = pairwise_dist\n",
    "    \n",
    "    return cost_mat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get distance between two datasets, (C_A - Cost Matrix of Dataset A, C_B - Cost Matrix of Dataset B, epsilon - regularization parameter)\n",
    "def results(C_A, C_B, epsilon):\n",
    "    import matplotlib.pylab as pl\n",
    "    from mpl_toolkits.mplot3d import Axes3D \n",
    "    \n",
    "    C_A /= C_A.max()\n",
    "    C_B /= C_B.max()\n",
    "\n",
    "    p = ot.unif(len(C_A))\n",
    "    q = ot.unif(len(C_B))\n",
    "    \n",
    "   \n",
    "    #regularized entropic gromov wasserstein calculation\n",
    "    gw, log = ot.gromov.entropic_gromov_wasserstein(\n",
    "        C_A, C_B, p, q, 'square_loss', epsilon=epsilon, log=True, verbose=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    print('Entropic Gromov-Wasserstein distances: ' + str(log['gw_dist']))\n",
    "\n",
    "\n",
    "    pl.figure(1, (10, 5))\n",
    "    \n",
    "    \n",
    "    #plot of optimal coupling matrix for regularized entropic GW\n",
    "    \n",
    "    pl.subplot(1, 2, 2)\n",
    "    pl.imshow(gw, cmap = 'nipy_spectral', interpolation='nearest')\n",
    "    pl.title('Entropic Gromov Wasserstein ($\\epsilon =$)', epsilon )\n",
    "    plt.grid(color='red', linestyle='-.', linewidth=0.7)\n",
    "    pl.show()\n",
    "    \n",
    "    distance = [gw, log]\n",
    "    return distance\n",
    "\n",
    "    \n",
    "#creating and randomizing dataframes for all datasets       \n",
    "   \n",
    "mnist = pd.concat([pd.DataFrame(mnistt.data), pd.DataFrame({ 'target' : mnistt.target})], axis = 1)\n",
    "\n",
    "mnist =shuffle(mnist, random_state = 2)\n",
    "\n",
    "fmnist = pd.concat([pd.DataFrame(fmnistt.data), pd.DataFrame({ 'target' : fmnistt.target})], axis = 1)\n",
    "\n",
    "fmnist =shuffle(fmnist, random_state = 2)\n",
    "\n",
    "images = images.reshape((images.shape[0], 784))\n",
    "emnist = pd.concat([pd.DataFrame(images), pd.DataFrame({ 'target' : labels})], axis = 1)\n",
    "emnist = shuffle(emnist, random_state = 2)\n",
    "\n",
    "kmnist_data = kmnist_data.reshape((kmnist_data.shape[0], 784))\n",
    "kmnist = pd.concat([pd.DataFrame(kmnist_data), pd.DataFrame({ 'target' : labels_kmnist})], axis = 1)\n",
    "kmnist = shuffle(kmnist, random_state = 2)   \n",
    "\n",
    "usps = pd.concat([pd.DataFrame(validation_usps), pd.DataFrame({ 'target' : validation_usps_label})], axis = 1)\n",
    "\n",
    "usps =shuffle(usps, random_state = 2)\n",
    "\n",
    "validation_usps = pd.DataFrame(usps.iloc[:,0:784]).to_numpy()\n",
    "\n",
    "validation_usps_label =usps.target\n",
    "\n",
    "data_crop_1 = data_cropp.reshape((data_cropp.shape[0], 196))\n",
    "\n",
    "mnist_crop = pd.concat([pd.DataFrame(data_crop_1), pd.DataFrame({ 'target' : mnistt.target})], axis = 1)\n",
    "\n",
    "mnist_crop =shuffle(mnist_crop, random_state = 2)\n",
    "\n",
    "dataset_crop = pd.DataFrame(mnist_crop.iloc[:,0:196]).to_numpy()\n",
    "\n",
    "labels_crop = mnist_crop.target\n",
    "\n",
    "data_60_1 = data_60.reshape((data_60.shape[0], 784))\n",
    "\n",
    "mnist_60 = pd.concat([pd.DataFrame(data_60_1), pd.DataFrame({ 'target' : mnistt.target})], axis = 1)\n",
    "\n",
    "mnist_60 =shuffle(mnist_60, random_state = 2)\n",
    "\n",
    "dataset_60 = pd.DataFrame(mnist_60.iloc[:,0:784]).to_numpy()\n",
    "\n",
    "labels_60 = mnist_60.target\n",
    "\n",
    "data_120_1 = data_120.reshape((data_120.shape[0], 784))\n",
    "\n",
    "mnist_120 = pd.concat([pd.DataFrame(data_120_1), pd.DataFrame({ 'target' : mnistt.target})], axis = 1)\n",
    "\n",
    "mnist_120 =shuffle(mnist_120, random_state = 2)\n",
    "\n",
    "dataset_120 = pd.DataFrame(mnist_120.iloc[:,0:784]).to_numpy()\n",
    "\n",
    "labels_120 = mnist_120.target\n",
    "\n",
    "data_180_1 = data_180.reshape((data_180.shape[0], 784))\n",
    "\n",
    "mnist_180 = pd.concat([pd.DataFrame(data_180_1), pd.DataFrame({ 'target' : mnistt.target})], axis = 1)\n",
    "\n",
    "mnist_180 =shuffle(mnist_180, random_state = 2)\n",
    "\n",
    "dataset_180 = pd.DataFrame(mnist_180.iloc[:,0:784]).to_numpy()\n",
    "\n",
    "labels_180 = mnist_180.target\n",
    "\n",
    "def main():\n",
    "     #calculating pairwise pointwise cost matrix for all datastes\n",
    "     cost_mat_mnist = cost_mat(mnistt.data, mnistt.target , mnist)\n",
    "     cost_mat_fmnist = cost_mat(fmnistt.data, fmnistt.target , fmnist)\n",
    "     cost_mat_emnist = cost_mat(images, labels , emnist)\n",
    "     cost_mat_kmnist = cost_mat(kmnist_data, labels_kmnist, kmnist)  \n",
    "     cost_mat_usps2 = cost_mat(validation_usps, validation_usps_label, usps)\n",
    "     cost_mat_crop = cost_mat(dataset_crop, labels_crop, mnist_crop)\n",
    "     cost_mat_60 = cost_mat(dataset_60, labels_60, mnist_60)\n",
    "     cost_mat_120 = cost_mat(dataset_120, labels_120, mnist_120)\n",
    "     cost_mat_180 = cost_mat(dataset_180, labels_180, mnist_180)\n",
    "        \n",
    "   \n",
    "  \n",
    "     #Plotting Figure 3 (matrix containing dataset to dataset OTDGWD)\n",
    "     sets_1 = [\"MNIST\", \"EMNIST\", \"FMNIST\", \"KMNIST\",\n",
    "              \"USPS\"]\n",
    "     sets_2 = [\"MNIST\", \"EMNIST\", \"FMNIST\", \"KMNIST\",\n",
    "              \"USPS\"]\n",
    "\n",
    "     entropicGWs = np.array([[0.0, float(str(results(cost_mat_emnist, cost_mat_mnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_mnist, cost_mat_fmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_mnist, cost_mat_kmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_mnist, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_emnist, cost_mat_mnist, 0.1)[1]['gw_dist'])[:5]), 0.0, float(str(results(cost_mat_emnist, cost_mat_fmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_emnist, cost_mat_kmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_emnist, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_fmnist, cost_mat_mnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_fmnist, cost_mat_emnist, 0.1)[1]['gw_dist'])[:5]), 0.0, float(str(results(cost_mat_fmnist, cost_mat_kmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_fmnist, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_kmnist, cost_mat_mnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_kmnist, cost_mat_emnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_kmnist, cost_mat_fmnist, 0.1)[1]['gw_dist'])[:5]), 0.0, float(str(results(cost_mat_kmnist, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_usps2, cost_mat_mnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_usps2, cost_mat_emnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_usps2, cost_mat_fmnist, 0.1)[1]['gw_dist'])[:5]), float(str(results(cost_mat_usps2, cost_mat_kmnist, 0.1)[1]['gw_dist'])[:5]), 0.0]])\n",
    "\n",
    "\n",
    "     fig, ax = plt.subplots()\n",
    "     im = ax.imshow(entropicGWs)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "     ax.set_xticks(np.arange(len(sets_2)))\n",
    "     ax.set_yticks(np.arange(len(sets_1)))\n",
    "    # ... and label them with the respective list entries\n",
    "     ax.set_xticklabels(sets_2)\n",
    "     ax.set_yticklabels(sets_1)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "     plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "     for i in range(len(sets_2)):\n",
    "         for j in range(len(sets_1)):\n",
    "             text = ax.text(j, i, entropicGWs[i, j],\n",
    "                        ha=\"center\", va=\"center\", color=\"b\")\n",
    "\n",
    "     ax.set_title(\"Dataset Distance\")\n",
    "     fig.tight_layout()\n",
    "     plt.show()\n",
    "    \n",
    "     #Plotting Figure 4 for epsilon = 0.1 (column vector containing  OTDGWD between augmented MNIST datasets and USPS for robustness analysis)\n",
    "    \n",
    "     sets_1_augment = [\"MNIST\", \"MNIST-60\", \"MNIST-120\", \"MNIST-180\", \"MNIST-CROP\"]\n",
    "     sets_2_augment = [\"USPS\"]\n",
    "\n",
    "     entropicGWs_augment = np.array([[float(str(results(cost_mat_mnist, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_60, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_120, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_180, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])],\n",
    "                    [float(str(results(cost_mat_crop, cost_mat_usps2, 0.1)[1]['gw_dist'])[:5])]])\n",
    "\n",
    "\n",
    "     fig, ax = plt.subplots()\n",
    "     im = ax.imshow(entropicGWs_augment)\n",
    "\n",
    "# We want to show all ticks...\n",
    "     ax.set_xticks(np.arange(len(sets_2_augment)))\n",
    "     ax.set_yticks(np.arange(len(sets_1_augment)))\n",
    "# ... and label them with the respective list entries\n",
    "     ax.set_xticklabels(sets_2_augment)\n",
    "     ax.set_yticklabels(sets_1_augment)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "     plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "     for i in range(len(sets_1_augment)):\n",
    "         for j in range(len(sets_2_augment)):\n",
    "            text = ax.text(j, i, entropicGWs_augment[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"b\")\n",
    "\n",
    "     ax.set_title(\"Dataset Distance\")\n",
    "     fig.tight_layout()\n",
    "     plt.show()\n",
    "    \n",
    "    #classification accuracy on target dataset using a neural network pre-trained on the source dataset (see Domain_Adaptation.ipynb for details and DA_results.txt for relevant results)\n",
    "     DA_accuracy = [0.9077056050300598, 0.9072727560997009, 0.9040692448616028, 0.9322309494018555, 0.9223484992980957, 0.9295114874839783,0.9868000149726868, 0.9918000102043152, 0.9889000058174133, 0.9828511476516724, 0.9790014028549194, 0.9824747443199158, 0.8898484706878662, 0.872943639755249, 0.8988449573516846, 0.9278963804244995 ]\n",
    "    #classification accuracy of the neural network with just training on the target dataset (see Domain_Adaptation.ipynb for details and DA_results.txt for relevant results)\n",
    "     source_accuracy = [0.9210822582244873, 0.9335178732872009,0.9939000010490417, 0.9831818342208862,  0.9810990691184998 ]\n",
    "    \n",
    "    #calculating transferability error as defined in the Final Report\n",
    "     DA_error = []\n",
    "     for h in range(len(DA_accuracy)):\n",
    "         DA_accuracy[h] = 1 - DA_accuracy[h]\n",
    "     \n",
    "     for j in range(len(source_accuracy)):\n",
    "         source_accuracy[j] = 1 - source_accuracy[j]\n",
    "     for i in range(len(DA_accuracy)):\n",
    "        \n",
    "         temp = 0\n",
    "         if i < 3:\n",
    "       \n",
    "             temp = 100*((DA_accuracy[i] - source_accuracy[0]) / (source_accuracy[0]))\n",
    "             DA_error.append(temp)\n",
    "         elif i >= 3 and i < 6:\n",
    "        \n",
    "             temp = 100*((DA_accuracy[i] - source_accuracy[1]) / (source_accuracy[1]))\n",
    "             DA_error.append(temp)\n",
    "         elif i >= 6 and i < 9:\n",
    "        \n",
    "             temp = 100*((DA_accuracy[i] - source_accuracy[2]) / (source_accuracy[2]))\n",
    "             DA_error.append(temp)\n",
    "        \n",
    "         elif i >= 9 and i < 12:\n",
    "             temp = 100*((DA_accuracy[i] - source_accuracy[3]) / (source_accuracy[3]))\n",
    "             DA_error.append(temp)\n",
    "         elif i >= 12 and i < 16:\n",
    "             temp = 100*((DA_accuracy[i] - source_accuracy[4]) / (source_accuracy[4]))\n",
    "             DA_error.append(temp)\n",
    "        \n",
    "    \n",
    "        \n",
    "    #Plotting Figure 2 for epsilon - 0.1\n",
    "     GWs_for_regression = np.array([results(cost_mat_mnist, cost_mat_fmnist, 0.1)[1]['gw_dist'], results(cost_mat_emnist, cost_mat_fmnist, 0.1)[1]['gw_dist'], results(cost_mat_kmnist, cost_mat_fmnist, 0.1)[1]['gw_dist'], results(cost_mat_mnist, cost_mat_emnist, 0.1)[1]['gw_dist'], results(cost_mat_fmnist, cost_mat_emnist, 0.1)[1]['gw_dist'], results(cost_mat_kmnist, cost_mat_emnist, 0.1)[1]['gw_dist'], results(cost_mat_fmnist, cost_mat_mnist, 0.1)[1]['gw_dist'], results(cost_mat_emnist, cost_mat_mnist, 0.1)[1]['gw_dist'], results(cost_mat_kmnist, cost_mat_mnist, 0.1)[1]['gw_dist'], results(cost_mat_mnist, cost_mat_kmnist, 0.1)[1]['gw_dist'], results(cost_mat_fmnist, cost_mat_kmnist, 0.1)[1]['gw_dist'], results(cost_mat_emnist, cost_mat_kmnist, 0.1)[1]['gw_dist'], results(cost_mat_mnist, cost_mat_usps2, 0.1)[1]['gw_dist'], results(cost_mat_fmnist, cost_mat_usps2, 0.1)[1]['gw_dist'], results(cost_mat_emnist, cost_mat_usps2, 0.1)[1]['gw_dist'], results(cost_mat_kmnist, cost_mat_usps2, 0.1)[1]['gw_dist']]) \n",
    "    \n",
    "    \n",
    "     df = pd.concat([pd.DataFrame(GWs_for_regression , columns = ['OTDGWD Distance Between Datasets']), pd.DataFrame(DA_error, columns = ['Relative Change in Classification Error'])], axis = 1)\n",
    "     \n",
    "     sns.set_style(\"white\")\n",
    "     gridobj = sns.lmplot(x='OTDGWD Distance Between Datasets', y='Relative Change in Classification Error', data=df, \n",
    "                     height=5, aspect=1.6, robust=True, palette='tab10', \n",
    "                     scatter_kws=dict(s=20, linewidths=.4, edgecolors='black'))\n",
    "     #point labels\n",
    "     n = ['M-F', 'E-F', 'K-F', 'M-E', 'F-E', 'K-E', 'F-M', 'E-M', 'K-M', 'M-K', 'F-K', 'E-K', 'M-U', 'F-U', 'E-U', 'K-U' ]\n",
    "\n",
    "    #generating plots between distance and transferability error\n",
    "\n",
    "     for i, txt in enumerate(n):\n",
    "        plt.annotate(txt, (df['OTDGWD Distance Between Datasets'][i], df['Relative Change in Classification Error'][i]), (df['OTDGWD Distance Between Datasets'][i], df['Relative Change in Classification Error'][i]))\n",
    "    \n",
    "     gridobj.set(xlim=(0.09, 0.1), ylim=(0, 20))\n",
    "     plt.show()\n",
    "\n",
    "     #calculating and printing corresponding p-value\n",
    "     X2 = sm.add_constant(GWs_for_regression)\n",
    "     est = sm.OLS(DA_error, X2)\n",
    "     print(est.fit().f_pvalue)\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
